{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL to NL to SQL architecture. \n",
    "\n",
    "In many cases, a company has already many anlytic assets such like SQLs, procedure, OLAP cubes and others in their firm. \n",
    "\n",
    "SQL is a valuable tool, but it can be challenging to use in a reusable way because it is not always clear what the query is doing.\n",
    "\n",
    "For example, \n",
    "\n",
    "If someone want to know 'which department has poor performace than the average ?'\n",
    "\n",
    "The goal is very simple, but its implementation SQL is very complex. \n",
    "\n",
    "There is a chasm between the business goal and the specific SQL implentation. \n",
    "\n",
    "In this case, pre-defined complex SQL can help to solve this problem. And we should suggest how to retrieve appropriate SQL from the lots of pre-defined SQLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pandas google-cloud-aiplatform google-cloud-bigquery\n",
    "#! pip install python-dotenv langchain\n",
    "#! pip install langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implment this function\n",
    "import pandas as pd\n",
    "\n",
    "def crawl_prefined_sqls():\n",
    "  sqls = []\n",
    "  # goal = \"SQL to check the delay time(second) before shipping for each order and product\"\n",
    "  sqls.append(\"\"\"\n",
    "CREATE TEMP FUNCTION convertIntervalToSecond(start_ts TIMESTAMP, end_ts TIMESTAMP)\n",
    "RETURNS INT64\n",
    "AS (\n",
    "  (EXTRACT(DAY FROM (end_ts - start_ts)) * 86400 +\n",
    "  EXTRACT(HOUR FROM (end_ts - start_ts)) * 3600 +\n",
    "  EXTRACT(MINUTE FROM (end_ts - start_ts)) * 60 +\n",
    "  EXTRACT(SECOND FROM (end_ts - start_ts)))\n",
    ");\n",
    "select convertIntervalToSecond(a.created_at, a.shipped_at) as order_pending_second, \n",
    "  convertIntervalToSecond(b.created_at, b.shipped_at) as product_pending_second, \n",
    "  a.status, a.order_id, a.user_id, a.gender, a.created_at, a.shipped_at, \n",
    "  b.product_id, b.created_at, b.shipped_at, b.delivered_at, b.returned_at\n",
    " from `bigquery-public-data.thelook_ecommerce.orders` a\n",
    " join `bigquery-public-data.thelook_ecommerce.order_items` b on (a.order_id = b.order_id)\n",
    " where a.status not in ('Cancelled') \n",
    "   and a.created_at between '2022-01-01' and '2022-06-30'\n",
    "order by a.order_id, b.product_id\n",
    "  \"\"\")\n",
    "  return sqls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain_google_vertexai import VertexAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PROJECT_ID=os.getenv(\"PROJECT_ID\")  # @param {type:\"string\"}\n",
    "LOCATION=os.getenv(\"LOCATION\")\n",
    "DATASET=os.getenv(\"DATASET\")\n",
    "TABLE_NAME=os.getenv(\"TABLE_NAME\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "\n",
    "llm_vertex = VertexAI(\n",
    "    #model_name=\"text-bison@latest\",\n",
    "    model_name=\"text-bison-32k@002\",\n",
    "    max_output_tokens=8000,\n",
    "    temperature=0,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    ")\n",
    "\n",
    "llm = llm_vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_json_response(llm_json_response) -> any:\n",
    "  #print('llm response:'+ response)\n",
    "  start_char = '['\n",
    "  end_char = ']'\n",
    "  if llm_json_response.find('[') == -1 or max(0,llm_json_response.find('{')) < llm_json_response.find('[') :\n",
    "    start_char = '{'\n",
    "    end_char = '}'\n",
    "  start_index = llm_json_response.find(start_char)\n",
    "  end_index = llm_json_response.rfind(end_char)\n",
    "  json_data = llm_json_response[start_index:end_index+1]\n",
    "  parsed_json = json.loads(json_data)\n",
    "  return parsed_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def parse_python_object(llm_python_object) -> any:\n",
    "  print('llm response:'+ llm_python_object)\n",
    "  if llm_python_object.find('{') == -1:\n",
    "    start_char = '['\n",
    "    end_char = ']'\n",
    "  elif llm_python_object.find('[') == -1 or llm_python_object.find('{') < llm_python_object.find('[') :\n",
    "    start_char = '{'\n",
    "    end_char = '}'\n",
    "  start_index = llm_python_object.find(start_char)\n",
    "  end_index = llm_python_object.rfind(end_char)\n",
    "  object_data = llm_python_object[start_index:end_index+1]\n",
    "  print(object_data)\n",
    "  parsed_object = ast.literal_eval(object_data)\n",
    "  return parsed_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def explain_sql(sql):\n",
    "  example_json = \"\"\"\n",
    "  {\n",
    "    \"business_goal\": \"explanation of the SQL\",\n",
    "    \"prepared_statement\": \"select convertIntervalToSecond(a.created_at, a.shipped_at) as order_pending_second from sample_table where created_at between ? and ?\",\n",
    "    \"filter_columns\": [\n",
    "      {\n",
    "        \"table_name\": \"sample_table\",\n",
    "        \"column_name\": \"created_at\",\n",
    "        \"business goal\": \"filter by order created date\",\n",
    "        \"operator\": \"between\",\n",
    "        \"column_type\" : \"TIMESTAMP\",\n",
    "        \"filter names\": [\"order_created_at_start\",\"order_created_at_end\"]\n",
    "        \"fitler_order\": 1\n",
    "      }\n",
    "    ],\n",
    "  }\n",
    "  \"\"\"\n",
    "  prompt_template = \"\"\"You are a server-side developer.\n",
    "Please convert the provided SQL to a prepared_statement and extract the filters from the SQL in JSON format. The filter_columns should only include predicate columns. Do not suggest Python code. Describe the business goal based on the prepared_statement.\n",
    "Step 1: Convert the provided SQL to a prepared_statement.\n",
    "Step 2: Extract the filters from the SQL in JSON format. The filter_columns should only include predicate columns. Do not suggest Python code.\n",
    "Step 3: Describe the business goal based on the prepared_statement.\n",
    "\n",
    "----------------------------\n",
    "example sql :\n",
    "select convertIntervalToSecond(a.created_at, a.shipped_at) as order_pending_second from sample_table where created_at between '2023-01-01' and '2023-12-01'\n",
    "\n",
    "example output json:\n",
    "{example_json}\n",
    "\n",
    "----------------------------\n",
    "provided sql:\n",
    "{sql}\n",
    "\n",
    "output json:\n",
    "\"\"\"\n",
    "  prompt = prompt_template.format(sql=sql, example_json=example_json)\n",
    "  response = llm.invoke(prompt)\n",
    "  print(response)\n",
    "  return parse_json_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```json\n",
      "{\n",
      "  \"business_goal\": \"This query aims to calculate the pending time (in seconds) for orders and products, and retrieve order-related information for orders that are not cancelled and were created between January 1, 2022, and June 30, 2022. The results are ordered by order ID and product ID.\",\n",
      "  \"prepared_statement\": \"CREATE TEMP FUNCTION convertIntervalToSecond(start_ts TIMESTAMP, end_ts TIMESTAMP)\\nRETURNS INT64\\nAS (\\n  (EXTRACT(DAY FROM (end_ts - start_ts)) * 86400 +\\n  EXTRACT(HOUR FROM (end_ts - start_ts)) * 3600 +\\n  EXTRACT(MINUTE FROM (end_ts - start_ts)) * 60 +\\n  EXTRACT(SECOND FROM (end_ts - start_ts)))\\n);\\nselect convertIntervalToSecond(a.created_at, a.shipped_at) as order_pending_second, \\n  convertIntervalToSecond(b.created_at, b.shipped_at) as product_pending_second, \\n  a.status, a.order_id, a.user_id, a.gender, a.created_at, a.shipped_at, \\n  b.product_id, b.created_at, b.shipped_at, b.delivered_at, b.returned_at\\n from `bigquery-public-data.thelook_ecommerce.orders` a\\n join `bigquery-public-data.thelook_ecommerce.order_items` b on (a.order_id = b.order_id)\\n where a.status not in (?) \\n   and a.created_at between ? and ?\\norder by a.order_id, b.product_id\",\n",
      "  \"filter_columns\": [\n",
      "    {\n",
      "      \"table_name\": \"orders\",\n",
      "      \"column_name\": \"status\",\n",
      "      \"business goal\": \"filter by order status\",\n",
      "      \"operator\": \"not in\",\n",
      "      \"column_type\" : \"STRING\",\n",
      "      \"filter names\": [\"order_status\"],\n",
      "      \"filter_order\": 1\n",
      "    },\n",
      "    {\n",
      "      \"table_name\": \"orders\",\n",
      "      \"column_name\": \"created_at\",\n",
      "      \"business goal\": \"filter by order created date\",\n",
      "      \"operator\": \"between\",\n",
      "      \"column_type\" : \"TIMESTAMP\",\n",
      "      \"filter names\": [\"order_created_at_start\",\"order_created_at_end\"],\n",
      "      \"filter_order\": 2\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "\n",
    "assetized_queries = []\n",
    "\n",
    "## In some cases, the LLM model can't extract the filter columns properly.\n",
    "\n",
    "for sql in crawl_prefined_sqls():\n",
    "  assetized_queries.append(explain_sql(sql))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "QueryJob<project=turnkey-charter-358922, location=asia-northeast3, id=18a7048c-d543-4c89-96e4-7aceb2aec7c0>\n"
     ]
    }
   ],
   "source": [
    "# from vector_util import VectorDatabase\n",
    "\n",
    "# vdb = VectorDatabase()\n",
    "\n",
    "# vdb.truncate_table()\n",
    "\n",
    "from bigquery_vector_util import BigQueryVectorDatabase, SqlSearchSchema\n",
    "\n",
    "vdb = BigQueryVectorDatabase(project_id=PROJECT_ID, location=LOCATION, dataset=DATASET, table_name=TABLE_NAME)\n",
    "vdb.create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/postgres/devel/looker_palm_integration/.venv/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.vertexai.VertexAIEmbeddings` was deprecated in langchain-community 0.0.12 and will be removed in 0.2.0. An updated version of the class exists in the langchain-google-vertexai package and should be used instead. To use it run `pip install -U langchain-google-vertexai` and import as `from langchain_google_vertexai import VertexAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "\n",
    "embeddings = VertexAIEmbeddings(\"textembedding-gecko-multilingual@latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     records\u001b[38;5;241m.\u001b[39mappend(SqlSearchSchema(sql\u001b[38;5;241m=\u001b[39msql, description\u001b[38;5;241m=\u001b[39mdescription, desc_vector\u001b[38;5;241m=\u001b[39mdesc_vector, parameters\u001b[38;5;241m=\u001b[39mparameters, explore_view\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, table_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, column_schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m      9\u001b[0m   vdb\u001b[38;5;241m.\u001b[39minsert_record(records)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mwrite_goal_to_vdb\u001b[49m\u001b[43m(\u001b[49m\u001b[43massetized_queries\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m, in \u001b[0;36mwrite_goal_to_vdb\u001b[0;34m(assetized_queries)\u001b[0m\n\u001b[1;32m      7\u001b[0m   desc_vector \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39membed_query(description)\n\u001b[1;32m      8\u001b[0m   records\u001b[38;5;241m.\u001b[39mappend(SqlSearchSchema(sql\u001b[38;5;241m=\u001b[39msql, description\u001b[38;5;241m=\u001b[39mdescription, desc_vector\u001b[38;5;241m=\u001b[39mdesc_vector, parameters\u001b[38;5;241m=\u001b[39mparameters, explore_view\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, table_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, column_schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m----> 9\u001b[0m \u001b[43mvdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecords\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/devel/looker_palm_integration/bigquery_vector_util.py:65\u001b[0m, in \u001b[0;36mBigQueryVectorDatabase.insert_record\u001b[0;34m(self, insertRecords)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minsert_record\u001b[39m(\u001b[38;5;28mself\u001b[39m, insertRecords: List[SqlSearchSchema]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m   records \u001b[38;5;241m=\u001b[39m [record\u001b[38;5;241m.\u001b[39mto_dict() \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m insertRecords]\n\u001b[1;32m     66\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbigquery_client\u001b[38;5;241m.\u001b[39minsert_rows_json(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtable_ref, records)\n\u001b[1;32m     67\u001b[0m   \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/devel/looker_palm_integration/bigquery_vector_util.py:65\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minsert_record\u001b[39m(\u001b[38;5;28mself\u001b[39m, insertRecords: List[SqlSearchSchema]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m   records \u001b[38;5;241m=\u001b[39m [\u001b[43mrecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m insertRecords]\n\u001b[1;32m     66\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbigquery_client\u001b[38;5;241m.\u001b[39minsert_rows_json(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtable_ref, records)\n\u001b[1;32m     67\u001b[0m   \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/devel/looker_palm_integration/bigquery_vector_util.py:28\u001b[0m, in \u001b[0;36mSqlSearchSchema.to_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     20\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msql\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters,\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplore_view\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplore_view,\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable_name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtable_name,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumn_schema\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_schema,\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdesc_vector\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc_vector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesc_vector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     29\u001b[0m   }\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "def write_goal_to_vdb(assetized_queries):\n",
    "  records = []\n",
    "  for assetized_query in assetized_queries:\n",
    "    description = assetized_query['business_goal']\n",
    "    sql = assetized_query['prepared_statement']\n",
    "    parameters = assetized_query['filter_columns']\n",
    "    desc_vector = embeddings.embed_query(description)\n",
    "    records.append(SqlSearchSchema(sql=sql, description=description, desc_vector=desc_vector, parameters=parameters, explore_view=None, model_name=None, table_name=None, column_schema=None))\n",
    "  vdb.insert_record(records)\n",
    "\n",
    "write_goal_to_vdb(assetized_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_query(question):\n",
    "  test_embedding =  embeddings.embed_query(question)\n",
    "  result = None\n",
    "  with vdb.get_connection() as conn:\n",
    "    try:\n",
    "      with conn.cursor() as cur:\n",
    "        select_record = (str(test_embedding).replace(' ',''),)\n",
    "        cur.execute(f\"SELECT sql, parameters, description FROM rag_test where (1 - (desc_vector <=> %s)) > 0.6 limit 1\", select_record)\n",
    "        if cur.rowcount == 0:\n",
    "          return None\n",
    "        rs = cur.fetchone()\n",
    "        result = {\n",
    "          'prepared_statement': rs[0],\n",
    "          'filter_columns': rs[1],\n",
    "          'description': rs[2]\n",
    "        }\n",
    "        print(result)\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to check the delay time(second) before shipping for each order and product?\"\n",
    "\n",
    "assetized_query = get_related_query(question)\n",
    "print(assetized_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_filter_values(assetized_query, question):\n",
    "  example_json = \"\"\"\n",
    "  {\n",
    "    \"filter_columns\": [\n",
    "      {\n",
    "        \"table_name\": \"sample_table\",\n",
    "        \"column_name\": \"col_1\",\n",
    "        \"operator\" : \"in\", \n",
    "        \"column_type\" : \"STRING\",\n",
    "        \"filter_names\": [\"col_1_filter\"],\n",
    "        \"filter_values\": [null],\n",
    "        \"filter_order\": 1\n",
    "      },\n",
    "      {\n",
    "        \"table_name\": \"sample_table\",\n",
    "        \"column_name\": \"col_2\",\n",
    "        \"operator\" : \"=\", \n",
    "        \"column_type\" : \"STRING\",\n",
    "        \"filter_names\": [\"col_2_filter\"],\n",
    "        \"filter_values\": [\"2022-01\"],\n",
    "        \"filter_order\": 2\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "  \"\"\"\n",
    "\n",
    "  prompt_template = \"\"\"You are a serverside developer. Extract filter values from the question and fill the values into the 'filter_values' item for the given filter columns. Do not suggest codes. Output should be the following JSON format.\n",
    "\n",
    "  given question:\n",
    "  {question}\n",
    "  \n",
    "  given sql:\n",
    "  {sql}\n",
    "\n",
    "  given filters:\n",
    "  {filters}\n",
    "\n",
    "  ----------------------------\n",
    "\n",
    "  output format(example) : json\n",
    "  {example_json}\n",
    "\n",
    "  \"\"\"\n",
    "  sql = assetized_query['prepared_statement']\n",
    "  filters = assetized_query['filter_columns']\n",
    "  prompt = prompt_template.format(sql=sql, filters=filters, example_json=example_json, question=question)\n",
    "  response = llm.predict(prompt)\n",
    "  return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_enriched = \"How to check the delay time(second) before shipping for each order and product in this year(2023)?\"\n",
    "\n",
    "response = extract_filter_values(assetized_query, question_enriched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_values = parse_json_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_unfilled_filters(filter_values):\n",
    "  unfilled_filters = []\n",
    "  for parameter in filter_values['filter_columns']:\n",
    "    if None in parameter['filter_values'] or len(parameter['filter_values']) == 0:\n",
    "      unfilled_filters.append(parameter)\n",
    "  return unfilled_filters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfilled_filters = check_unfilled_filters(filter_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfilled_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_response_to_fill_filter_values(unfilled_filters, question):\n",
    "  example_json = \"\"\"\n",
    "  {\n",
    "    \"agent_response\": \"...\"\n",
    "  }\n",
    "  \"\"\"\n",
    "  prompt_template = \"\"\"You are an automatic agent to serve natural language to SQL conversion. Please guide the user to fill the missing filter values in the given question and unfilled filters. Output should be the following JSON format.\n",
    "\n",
    "  question:\n",
    "  {question}\n",
    "\n",
    "  unfilled filters:\n",
    "  {unfilled_filters}\n",
    "\n",
    "  output format json:\n",
    "  {example_json}\n",
    "  \"\"\"\n",
    "  prompt = prompt_template.format(unfilled_filters=unfilled_filters, question=question, example_json=example_json)\n",
    "  response = llm.predict(prompt)\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = make_response_to_fill_filter_values(unfilled_filters, question=question_enriched)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_response = \"I want to know not cancelled orders only\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_filter_values_with_additional_response(unfilled_filters, additional_response):\n",
    "  example_json = \"\"\"\n",
    "  {\n",
    "    \"filter_columns\": [\n",
    "      {\n",
    "        \"column_name\": \"col_1\",\n",
    "        \"operator\" : \"in\", \n",
    "        \"column_type\" : \"STRING\",\n",
    "        \"filter_names\": [\"col_1_filter\"],\n",
    "        \"filter_values\": [null]\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "  \"\"\"\n",
    "\n",
    "  prompt_template = \"\"\"You are a serverside developer. Extract filter values from the user response and unfilled filter information. Do not suggest codes. Output should be the following JSON format.\n",
    "\n",
    "  output format json:\n",
    "  {example_json}\n",
    "\n",
    "  ----------------------------\n",
    "  user response :\n",
    "  {user_response}\n",
    "\n",
    "  unfilled filters:\n",
    "  {unfilled_filters}\n",
    "  \"\"\"\n",
    "  prompt = prompt_template.format(unfilled_filters=unfilled_filters, user_response=additional_response, example_json=example_json)\n",
    "  response = llm.predict(prompt)\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_filter_values = parse_json_response(extract_filter_values_with_additional_response(unfilled_filters, additional_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_filter_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_filter_values(filter_values, additional_filter_values):\n",
    "  original_parameters = filter_values['filter_columns']\n",
    "  additional_parameters = additional_filter_values['filter_columns']\n",
    "  for org_parameter in original_parameters:\n",
    "    for add_parameter in additional_parameters:\n",
    "      if org_parameter['column_name'] == add_parameter['column_name']:\n",
    "        org_parameter['filter_values'] = add_parameter['filter_values']\n",
    "  return filter_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_filter_values = merge_filter_values(filter_values, additional_filter_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_filter_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, LLM can handle extraction / tranformation and validation as well. \n",
    "\n",
    "Next step is very similar with 'Direction Conversion'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_field_unique_values(matched_table, matched_field):\n",
    "  if matched_table[0] != '`' :\n",
    "    matched_table = '`' + matched_table + '`'\n",
    "  sql_query = f\"with distinct_values as ( select distinct {matched_field} as {matched_field} from {matched_table} ) select {matched_field}, (select count(1) from distinct_values) as total_count from distinct_values limit 500\"\n",
    "  df = client.query(sql_query).to_dataframe()\n",
    "  return df[matched_field].tolist(), df['total_count'][0]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "def choose_right_filter_value(filter_values, wanted_value):\n",
    "  prompt_template = \"\"\"As a looker developer, choose right filter value for the wanted value below without changing filter value itself.\n",
    "\n",
    "  filter_values : {filter_values}\n",
    "\n",
    "  wanted_values: {wanted_value}\n",
    "\n",
    "  answer format: python list\n",
    "[filter_value1, filter_value2, ...]\n",
    "  \"\"\"\n",
    "  response = llm.predict(prompt_template.format(filter_values=filter_values, wanted_value=wanted_value))\n",
    "  return response \n",
    "\n",
    "def adjust_filter_value(filter_columns):\n",
    "  for filter in filter_columns:\n",
    "    matched_table = filter['table_name']\n",
    "    matched_field = filter['column_name']\n",
    "    filter['unique_values'], filter['unique_count'] = get_field_unique_values(matched_table, matched_field)\n",
    "    # TODO: if unique_count < 500, then choose right filter value in the unique value list.\n",
    "    if filter['unique_count'] < 500:\n",
    "      response = choose_right_filter_value(filter['unique_values'], filter['filter_values'])\n",
    "      if response.strip().find(\"```json\") == 0 :\n",
    "        filter['adjust_filter_values'] = parse_json_response(response)\n",
    "      else:\n",
    "        filter['adjust_filter_values'] = parse_python_object(response)\n",
    "    else:\n",
    "      filter['adjust_filter_values'] = filter['filter_values']\n",
    "    filter['unique_values'] = None\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjust_filter_value(merged_filter_values['filter_columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_filter_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepared_statement_with_filter_values_in_bigquery(sql_and_filters):\n",
    "  prepared_statement = sql_and_filters['prepared_statement']\n",
    "  query_parameters = []\n",
    "  for filter_column in sql_and_filters['filter_columns']:\n",
    "    if len(filter_column['adjust_filter_values']) > 1:\n",
    "      if len(filter_column['filter_names']) > 1:\n",
    "        for filter_value in filter_column['adjust_filter_values']:\n",
    "          if(filter_column['column_type'] == 'FLOAT64'):\n",
    "            query_parameters.append(bigquery.ScalarQueryParameter(None, \"FLOAT64\", filter_value))\n",
    "          elif(filter_column['column_type'] == 'INT64'):\n",
    "            query_parameters.append(bigquery.ScalarQueryParameter(None, \"INT64\", filter_value))\n",
    "          else:\n",
    "            query_parameters.append(bigquery.ScalarQueryParameter(None, \"STRING\", filter_value))  \n",
    "      else:\n",
    "        if(filter_column['column_type'] == 'FLOAT64'):\n",
    "          query_parameters.append(bigquery.ArrayQueryParameter(None, \"FLOAT64\", filter_column['adjust_filter_values']))\n",
    "        elif(filter_column['column_type'] == 'INT64'):\n",
    "          query_parameters.append(bigquery.ArrayQueryParameter(None, \"INT64\", filter_column['adjust_filter_values']))\n",
    "        else:\n",
    "          query_parameters.append(bigquery.ArrayQueryParameter(None, \"STRING\", filter_column['adjust_filter_values']))\n",
    "    else:\n",
    "      if(filter_column['column_type'] == 'FLOAT64'):\n",
    "        query_parameters.append(bigquery.ScalarQueryParameter(None, \"FLOAT64\", filter_column['adjust_filter_values'][0]))\n",
    "      elif(filter_column['column_type'] == 'INT64'):\n",
    "        query_parameters.append(bigquery.ScalarQueryParameter(None, \"INT64\", filter_column['adjust_filter_values'][0]))\n",
    "      else:\n",
    "        query_parameters.append(bigquery.ScalarQueryParameter(None, \"STRING\", filter_column['adjust_filter_values'][0]))  \n",
    "  job_config = bigquery.QueryJobConfig(\n",
    "    query_parameters=query_parameters\n",
    "  )\n",
    "  print(query_parameters)\n",
    "  query_job = client.query(prepared_statement, job_config=job_config)\n",
    "  return query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_and_filters = {\n",
    "  'prepared_statement':assetized_query['prepared_statement'],\n",
    "  'filter_columns': merged_filter_values['filter_columns']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_and_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = prepared_statement_with_filter_values_in_bigquery(sql_and_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
